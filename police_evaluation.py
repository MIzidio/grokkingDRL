import numpy as np

P = {
    0: {
        0: [(1.0, 0, 0.0, True)],
        1: [(1.0, 0, 0.0, True)]
    },
    1: {
        0: [(0.5000000000000001, 0, 0.0, True),
            (0.3333333333333333, 1, 0.0, False),
            (0.16666666666666666, 2, 0.0, False)
        ],
        1: [(0.5000000000000001, 2, 0.0, False),
            (0.3333333333333333, 1, 0.0, False),
            (0.16666666666666666, 0, 0.0, True)
        ]
    },
    2: {
        0: [(0.5000000000000001, 1, 0.0, False),
            (0.3333333333333333, 2, 0.0, False),
            (0.16666666666666666, 3, 0.0, False)
        ],
        1: [(0.5000000000000001, 3, 0.0, False),
            (0.3333333333333333, 2, 0.0, False),
            (0.16666666666666666, 1, 0.0, False)
        ]
    },
    3: {
        0: [(0.5000000000000001, 2, 0.0, False),
            (0.3333333333333333, 3, 0.0, False),
            (0.16666666666666666, 4, 0.0, False)
        ],
        1: [(0.5000000000000001, 4, 0.0, False),
            (0.3333333333333333, 3, 0.0, False),
            (0.16666666666666666, 2, 0.0, False)
        ]
    },
    4: {
        0: [(0.5000000000000001, 3, 0.0, False),
            (0.3333333333333333, 4, 0.0, False),
            (0.16666666666666666, 5, 0.0, False)
        ],
        1: [(0.5000000000000001, 5, 0.0, False),
            (0.3333333333333333, 4, 0.0, False),
            (0.16666666666666666, 3, 0.0, False)
        ]
    },
    5: {
        0: [(0.5000000000000001, 4, 0.0, False),
            (0.3333333333333333, 5, 0.0, False),
            (0.16666666666666666, 6, 1.0, True)
        ],
        1: [(0.5000000000000001, 6, 1.0, True),
            (0.3333333333333333, 5, 0.0, False),
            (0.16666666666666666, 4, 0.0, False)
        ]
    },
    6: {
        0: [(1.0, 6, 0.0, True)],
        1: [(1.0, 6, 0.0, True)]
    }
}

LEFT, RIGHT = range(2)
pi = lambda s: {
    0:LEFT, 1:LEFT, 2:LEFT, 3:LEFT, 4:LEFT, 5:LEFT, 6:LEFT
}[s]

def policy_evaluation(pi, P, gamma=1.0, theta=1e-10):
    prev_V = np.random.rand(len(P))

    while True:
        V = np.random.rand(len(P))

        for s in range(len(P)):
            for prob, next_state, reward, done in P[s][pi(s)]:
                V[s] += prob * (reward + gamma * prev_V[next_state] * (not done))
        if (np.max(np.abs(prev_V - V)) < theta):
            break
        prev_V = V.copy()

    return prev_V

V = policy_evaluation(pi, P)
print(V)